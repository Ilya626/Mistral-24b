# --- DARE-Linear (профиль по глубине) ---
merge_method: dare_linear
base_model: &BASE_MODEL /workspace/Mistral-Small-3.2-24B-Instruct-2506
dtype: bfloat16

models:
  - model: &VISTRAL /workspace/Vistral-24B-Instruct
    base_model: *BASE_MODEL
    layer_range: [0, 39]  # профиль рассчитан на 40 слоёв (индексы включительные); проверьте при замене модели
    # Вклад Vistral: больше внизу, меньше к верху
    parameters:
      weight:
        - filter: default
          value: [0.50, 0.50, 0.50, 0.50, 0.50]
        - filter: self_attn
          value: [0.65, 0.60, 0.55, 0.50, 0.45]
        - filter: mlp
          value: [0.60, 0.55, 0.50, 0.45, 0.40]
        - tensors:
            - model.embed_tokens.weight
            - lm_head.weight
          value: 0.50
  - model: &CYDONIA /workspace/Cydonia-24B-v4.2.0
    base_model: *BASE_MODEL
    layer_range: [0, 39]
    # Вклад Cydonia: наоборот — меньше внизу, больше к верху
    parameters:
      weight:
        - filter: default
          value: [0.50, 0.50, 0.50, 0.50, 0.50]
        - filter: self_attn
          value: [0.35, 0.40, 0.45, 0.50, 0.55]
        - filter: mlp
          value: [0.40, 0.45, 0.50, 0.55, 0.60]
        - tensors:
            - model.embed_tokens.weight
            - lm_head.weight
          value: 0.50

parameters:
  # Разреживание дельт DARE: можно тоже «профилировать» и/или
  # делать его отличным для attn и mlp (значения — примерные)
  density:
    - filter: default
      value: [0.60, 0.58, 0.58, 0.55, 0.55]
    - filter: self_attn
      value: [0.65, 0.60, 0.60, 0.55, 0.55]
    - filter: mlp
      value: [0.60, 0.55, 0.55, 0.50, 0.50]
    - tensors:
        - model.embed_tokens.weight
        - lm_head.weight
      value: 1.0
  normalize: true   # веса нормализуются автоматически

# Словарь объединяем, чтобы сохранить +2 токена Vistral
tokenizer_source: union
