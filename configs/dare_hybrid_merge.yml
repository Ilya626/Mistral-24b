# --- Холст для слияния ---
base_model: /workspace/Vistral-24B-Instruct
dtype: bfloat16
merge_method: passthrough   # требуется валидатору; ниже переопределяем по срезам

slices:
  # 1.1 SLERP общей части эмбеддингов (0..131072)
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        layer_range: [0, 0]
        tensor_slice: [0, 131072]
      - model: /workspace/Cydonia-24B-v4.2.0
        layer_range: [0, 0]
        tensor_slice: [0, 131072]
    destinations:
      - model.embed_tokens.weight
    merge_method: slerp
    parameters:
      t: 0.5

  # 1.1b SLERP общей части lm_head (0..131072)
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        layer_range: [0, 0]
        tensor_slice: [0, 131072]
      - model: /workspace/Cydonia-24B-v4.2.0
        layer_range: [0, 0]
        tensor_slice: [0, 131072]
    destinations:
      - lm_head.weight
    merge_method: slerp
    parameters:
      t: 0.5

  # 1.2 Копия двух «лишних» токенов Vistral в embed_tokens (131072..131074)
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        layer_range: [0, 0]
        tensor_slice: [131072, 131074]
    destinations:
      - model.embed_tokens.weight
    merge_method: passthrough

  # 1.3 Копия двух «лишних» токенов Vistral в lm_head (131072..131074)
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        layer_range: [0, 0]
        tensor_slice: [131072, 131074]
    destinations:
      - lm_head.weight
    merge_method: passthrough

  # 2. Тело модели — DARE TIES только по слоям трансформера
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        base_model: /workspace/Mistral-Small-3.1-24B-Base-2503
        layer_range: [0, 39]     # проверь фактическое число слоёв
      - model: /workspace/Cydonia-24B-v4.2.0
        base_model: /workspace/Mistral-Small-3.1-24B-Base-2503
        layer_range: [0, 39]
    include:
      - '^model\\.layers\\..*'
    exclude:
      - model.embed_tokens.weight
      - lm_head.weight
    merge_method: dare_ties
    parameters:
      density: 0.5
      weight: 1.0

  # 3. Всё остальное — копируем из Vistral (строго 1→1 для passthrough)
  - sources:
      - model: /workspace/Vistral-24B-Instruct
        layer_range: [0, 0]
    include:
      - '^(?!model\\.layers\\.).*'
    exclude:
      - model.embed_tokens.weight
      - lm_head.weight
    merge_method: passthrough

# Вокаб — объединяем, чтобы не было тримминга и сохранить 2 доп. токена
tokenizer:
  source: union
  # при желании можно выровнять кратность:
  # pad_to_multiple_of: 8
